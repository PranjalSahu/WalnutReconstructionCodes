{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] All the Imports\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from pathlib import Path\n",
    "import ast\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "import cv2\n",
    "\n",
    "import random\n",
    "import glob\n",
    "\n",
    "import csv\n",
    "from scipy import ndimage, misc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     2,
     9,
     29,
     53
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] Attribute and Category Model\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "class MyAttrCateModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model    = models.resnet18(pretrained=True)\n",
    "        self.model.fc = Identity()\n",
    "        \n",
    "        #self.attr_layer = nn.Sequential(nn.Linear(512, 128, bias=False), \n",
    "        #                                nn.ReLU(inplace=True),\n",
    "        #                                nn.Linear(128, 26, bias=False)\n",
    "        #                               )\n",
    "        \n",
    "        #self.cate_layer = nn.Sequential(nn.Linear(512, 128, bias=False), \n",
    "        #                                nn.ReLU(inplace=True),\n",
    "        #                                nn.Linear(128, 50, bias=False))\n",
    "        self.attr_layer = nn.Linear(512, 26)\n",
    "        self.cate_layer = nn.Linear(512, 50)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out1     = self.model(x)\n",
    "        attr_out = self.attr_layer(out1)\n",
    "        cate_out = self.cate_layer(out1)\n",
    "        #cate_out = torch.flatten(cate_out)\n",
    "        return attr_out, cate_out\n",
    "\n",
    "class MyAttrCateModel50(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model    = models.resnet50(pretrained=True)\n",
    "        self.model.fc = Identity()\n",
    "        \n",
    "        #self.attr_layer = nn.Sequential(nn.Linear(2048, 128, bias=False), \n",
    "        #                                nn.ReLU(inplace=True),\n",
    "        #                                nn.Linear(128, 26, bias=False))\n",
    "        #self.cate_layer = nn.Sequential(nn.Linear(2048, 128, bias=False), \n",
    "        #                                nn.ReLU(inplace=True),\n",
    "        #                                nn.Linear(128, 50, bias=False))\n",
    "        self.attr_layer = nn.Linear(2048, 26)\n",
    "        self.cate_layer = nn.Linear(2048, 50)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out1     = self.model(x)\n",
    "        attr_out = self.attr_layer(out1)\n",
    "        cate_out = self.cate_layer(out1)\n",
    "        #cate_out = torch.flatten(cate_out)\n",
    "        return attr_out, cate_out\n",
    "#model  = MyAttrCateModel()\n",
    "# x      = torch.randn(1, 3, 224, 224)\n",
    "# output = model(x)\n",
    "# print(output[0].shape, output[1].shape)\n",
    "\n",
    "#print(model)\n",
    "#model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     15
    ]
   },
   "outputs": [],
   "source": [
    "# [CLEAN CODE]\n",
    "\n",
    "from __future__ import division\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "\n",
    "class AttrDataset(Dataset):\n",
    "    CLASSES = None\n",
    "    \n",
    "    def __init__(self,\n",
    "                 img_path,\n",
    "                 img_file,\n",
    "                 label_file,\n",
    "                 cate_file,\n",
    "                 bbox_file,\n",
    "                 landmark_file,\n",
    "                 img_size,\n",
    "                 idx2id=None):\n",
    "        self.img_path = img_path\n",
    "\n",
    "        normalize = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(img_size[0]),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "        # read img names\n",
    "        fp = open(img_file, 'r')\n",
    "        self.img_list = [x.strip() for x in fp]\n",
    "\n",
    "        # read attribute labels and category annotations\n",
    "        self.labels = np.loadtxt(label_file, dtype=np.float32)\n",
    "\n",
    "        # read categories\n",
    "        self.categories = []\n",
    "        catefn = open(cate_file).readlines()\n",
    "        for i, line in enumerate(catefn):\n",
    "            self.categories.append(line.strip('\\n'))\n",
    "\n",
    "        self.img_size = img_size\n",
    "    \n",
    "    def get_basic_item(self, idx):\n",
    "        print(os.path.join(self.img_path, self.img_list[idx]))\n",
    "        img = Image.open(os.path.join(self.img_path,\n",
    "                                      self.img_list[idx])).convert('RGB')\n",
    "\n",
    "        width, height  = img.size\n",
    "        print('Original Image size is ', width, height)\n",
    "        # Very Important\n",
    "        # For getting the cropped and resized region of interest image\n",
    "        img.thumbnail(self.img_size, Image.ANTIALIAS)\n",
    "        img   = self.transform(img)\n",
    "\n",
    "        label    = torch.from_numpy(self.labels[idx])\n",
    "        cate     = torch.LongTensor([int(self.categories[idx]) - 1])\n",
    "\n",
    "        data = {'img': img, 'attr': label, 'cate': cate}\n",
    "        return data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.get_basic_item(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "\n",
    "class ValidAttrDataset(Dataset):\n",
    "    CLASSES = None\n",
    "    \n",
    "    def __init__(self,\n",
    "                 img_path,\n",
    "                 img_file,\n",
    "                 label_file,\n",
    "                 cate_file,\n",
    "                 bbox_file,\n",
    "                 landmark_file,\n",
    "                 img_size,\n",
    "                 idx2id=None):\n",
    "        self.img_path = img_path\n",
    "\n",
    "        normalize = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(img_size[0]),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "        # read img names\n",
    "        fp = open(img_file, 'r').read().split('\\n')\n",
    "        self.img_list = fp#[x.strip() for x in fp]\n",
    "        \n",
    "        self.img_size = img_size\n",
    "    \n",
    "    def get_basic_item(self, idx):\n",
    "        try:\n",
    "            #print(os.path.join(self.img_path, self.img_list[idx]))\n",
    "            img = Image.open(os.path.join(self.img_path,\n",
    "                                          self.img_list[idx])).convert('RGB')\n",
    "\n",
    "            width, height  = img.size\n",
    "            img.resize((width//2, height//2))\n",
    "            width, height  = img.size\n",
    "\n",
    "            #print('Original Image size is ', width, height)\n",
    "            # Very Important\n",
    "            # For getting the cropped and resized region of interest image\n",
    "            img.thumbnail(self.img_size, Image.ANTIALIAS)\n",
    "            img   = self.transform(img)\n",
    "\n",
    "            #label    = torch.from_numpy(self.labels[idx])\n",
    "            #cate     = torch.LongTensor([int(self.categories[idx]) - 1])\n",
    "\n",
    "            data = {'img': img, 'imgpath': os.path.join(self.img_path,\n",
    "                                          self.img_list[idx])}#, 'attr': label, 'cate': cate}\n",
    "            return data\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.get_basic_item(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "img_path   = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Img/\"\n",
    "img_file   = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/train.txt\"\n",
    "label_file = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/train_attr.txt\"\n",
    "cate_file  = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/train_cate.txt\"\n",
    "img_size   = [224, 224]\n",
    "#img_size   = [256, 256]\n",
    "\n",
    "landmark_file = None\n",
    "bbox_file     = None\n",
    "\n",
    "img_file   = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/val.txt\"\n",
    "label_file = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/val_attr.txt\"\n",
    "cate_file  = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/val_cate.txt\"\n",
    "\n",
    "\n",
    "# Change these as per your need\n",
    "img_path   = \"/home/yu-hao/Downloads/30 Styles-20210216T225758Z-001/30 Styles/\"\n",
    "img_file   = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/newval.txt\"\n",
    "\n",
    "d3 = ValidAttrDataset(img_path, img_file, label_file, cate_file, bbox_file, landmark_file, img_size, idx2id=None)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def build_dataloader(dataset, batch_size, shuffle):\n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=0,\n",
    "        pin_memory=False)\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "model  = MyAttrCateModel50()\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "params       = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer    = torch.optim.Adam(params, lr=0.0001, weight_decay=0.0001)\n",
    "lr_scheduler = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:     Shorts [['solid' 'sleeveless' 'no_dress' 'no_neckline' 'cotton' 'conventional']]\n"
     ]
    }
   ],
   "source": [
    "# [STAR] MMFASHION Testing on a single image\n",
    "\n",
    "attr_list = []\n",
    "attr_list_file = open(\"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/list_attr_cloth.txt\").read().split('\\n')\n",
    "for t in attr_list_file[2:-1]:\n",
    "    attr_list.append(t.split()[0])\n",
    "attr_list = np.array(attr_list)\n",
    "\n",
    "cate_list = []\n",
    "cate_list_file = open(\"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/list_category_cloth.txt\").read().split('\\n')\n",
    "for t in cate_list_file[2:-1]:\n",
    "    cate_list.append(t.split()[0])\n",
    "cate_list = np.array(cate_list)\n",
    "\n",
    "model.load_state_dict(torch.load('fashion_cate_attr_resnet50_single_linear.pth'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "index = random.randint(0, len(d3))\n",
    "t1    = d3[index]\n",
    "\n",
    "new_images  = torch.Tensor(np.expand_dims(t1['img'], 0)).to(device)\n",
    "#attr_target = t1['attr'].to(device)\n",
    "#cate_target = t1['cate'].to(device)\n",
    "        \n",
    "out1, out2  = model(new_images)\n",
    "        \n",
    "out1 = torch.sigmoid(out1)\n",
    "out2 = torch.softmax(out2, axis=1)\n",
    "\n",
    "out1 = out1.data.cpu().numpy().flatten()\n",
    "out2 = out2.data.cpu().numpy().flatten()\n",
    "\n",
    "out1[out1 < 0.5] = 0\n",
    "out1 = np.array(out1.flatten())\n",
    "\n",
    "attr_index         = np.array(np.nonzero(out1))\n",
    "#attr_ground_index  = np.array(np.nonzero(t1['attr']).flatten())\n",
    "\n",
    "cate_index         = np.argmax(out2)\n",
    "#cate_ground_index  = t1['cate'].data.cpu().numpy()#[0][0]\n",
    "\n",
    "#print(cate_index, cate_ground_index)\n",
    "print(\"Predicted:    \", cate_list[cate_index], attr_list[attr_index])\n",
    "#print(\"Ground Truth: \", cate_list[cate_ground_index], attr_list[attr_ground_index])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
